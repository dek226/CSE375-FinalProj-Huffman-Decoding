{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e736f6-c820-475f-bdab-b67dc8e53d75",
   "metadata": {},
   "source": [
    "GPU Parallel Huffman Encoding/Decoding Project\n",
    "\n",
    "This Jupyter Notebook uses Numba and CuPy, the recommended tools for GPU computing in the RAPIDS environment.\n",
    "\n",
    "1. Setup and Utility Functions\n",
    "\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261605e-4229-4c91-b689-1e34cc3e1eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from numba import cuda\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfab7c4-42a1-454c-a705-39dda4b034f0",
   "metadata": {},
   "source": [
    "Configuration (Adjust these based on your tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c070844-1135-4bc7-aaf3-7788e2a215d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "THREAD_PER_BLOCK = 256\n",
    "MAX_SYMBOLS = 256 # For byte-level compression\n",
    "MAX_CODE_LENGTH = 32 # Maximum bits for a Huffman code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca031813-0499-4aba-98c6-8930c1151515",
   "metadata": {},
   "source": [
    "--- Utility Functions ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666106a9-4bce-43ea-b746-ab8077dad6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(size_mb, redundancy_level='medium'):\n",
    "\"\"\"Generates a byte array of a specific size with varying redundancy.\"\"\"\n",
    "size_bytes = size_mb * 1024 * 1024\n",
    "\n",
    "if redundancy_level == 'low':\n",
    "    # Near-uniform distribution (high entropy)\n",
    "    return np.random.randint(0, MAX_SYMBOLS, size=size_bytes, dtype=np.uint8)\n",
    "elif redundancy_level == 'high':\n",
    "    # Highly skewed distribution (low entropy)\n",
    "    popular_symbols = np.random.choice(np.arange(MAX_SYMBOLS), size=int(MAX_SYMBOLS * 0.1), replace=False)\n",
    "    weights = np.zeros(MAX_SYMBOLS)\n",
    "    weights[popular_symbols] = np.random.rand(len(popular_symbols))\n",
    "    weights /= weights.sum()\n",
    "    return np.random.choice(np.arange(MAX_SYMBOLS), size=size_bytes, p=weights, replace=True).astype(np.uint8)\n",
    "else: # medium (standard text-like distribution)\n",
    "    # 10 characters are frequent, others are less frequent\n",
    "    weights = np.array([0.05] * 10 + [0.0038] * (MAX_SYMBOLS - 10))\n",
    "    weights[0] += 1.0 - weights.sum() # Normalize\n",
    "    weights = np.maximum(weights, 0.0001) # Ensure no zero weights\n",
    "    weights /= weights.sum()\n",
    "    return np.random.choice(np.arange(MAX_SYMBOLS), size=size_bytes, p=weights, replace=True).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ff8ca-bc3c-4352-9cdd-2ddec9548358",
   "metadata": {},
   "source": [
    "2. Sequential Baseline Implementation (CPU)\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af983c87-9d45-46e8-a8a5-d4beebb6dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_huffman_encode(data):\n",
    "\"\"\"\n",
    "Standard sequential Huffman encoding (all stages combined) on CPU.\n",
    "Returns: code_table, compressed_bits, original_size\n",
    "\"\"\"\n",
    "# 1. Frequency Counting (Sequential)\n",
    "freq = Counter(data)\n",
    "\n",
    "# 2. Code Assignment (Sequential Tree Construction)\n",
    "if not freq:\n",
    "    return {}, b'', 0\n",
    "\n",
    "# Build the Huffman Tree\n",
    "heap = [[weight, [symbol, \"\"]] for symbol, weight in freq.items()]\n",
    "heapq.heapify(heap)\n",
    "\n",
    "while len(heap) > 1:\n",
    "    lo = heapq.heappop(heap)\n",
    "    hi = heapq.heappop(heap)\n",
    "    for pair in lo[1:]:\n",
    "        pair[1] = '0' + pair[1]\n",
    "    for pair in hi[1:]:\n",
    "        pair[1] = '1' + pair[1]\n",
    "    heapq.heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])\n",
    "    \n",
    "code_table = dict(lo[1:][i] for i in range(len(lo[1:])))\n",
    "\n",
    "# 3. Bitstream Encoding (Sequential)\n",
    "encoded_bits = \"\"\n",
    "for symbol in data:\n",
    "    encoded_bits += code_table[symbol]\n",
    "    \n",
    "# Pack bits into bytes\n",
    "padding_bits = 8 - (len(encoded_bits) % 8)\n",
    "encoded_bits += '0' * padding_bits\n",
    "\n",
    "# Header: Store padding info and code table\n",
    "header = str(padding_bits) + '|' + str(code_table) + '|'\n",
    "\n",
    "compressed_bytes = bytearray()\n",
    "for i in range(0, len(encoded_bits), 8):\n",
    "    byte = encoded_bits[i:i+8]\n",
    "    compressed_bytes.append(int(byte, 2))\n",
    "    \n",
    "return header.encode('utf-8') + bytes(compressed_bytes), len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bae1d6-7eac-4d04-b3c4-a0ae644e59bb",
   "metadata": {},
   "source": [
    "3. Parallel Implementation (GPU Kernels)\n",
    "\n",
    "==============================================================================\n",
    "\n",
    "--- Stage 1: Parallel Frequency Counting (CUDA Kernel) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5958b-7a07-485b-b134-3eaf4c9807d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def parallel_frequency_count(data, freq_out):\n",
    "\"\"\"\n",
    "Parallel Frequency Counting using a global atomic operation.\n",
    "Each thread processes one byte and increments the corresponding frequency count.\n",
    "\"\"\"\n",
    "idx = cuda.grid(1)\n",
    "if idx < data.size:\n",
    "# Use atomic add to safely increment the frequency count for the symbol\n",
    "# data[idx] is the symbol (byte value 0-255)\n",
    "# freq_out is the 256-element array storing counts\n",
    "cuda.atomic.add(freq_out, data[idx], 1)\n",
    "\n",
    "def gpu_frequency_stage(data_gpu, freq_out_gpu):\n",
    "\"\"\"Wrapper to launch the frequency counting kernel.\"\"\"\n",
    "blocks_per_grid = (data_gpu.size + THREAD_PER_BLOCK - 1) // THREAD_PER_BLOCK\n",
    "parallel_frequency_count[blocks_per_grid, THREAD_PER_BLOCK](data_gpu, freq_out_gpu)\n",
    "cuda.synchronize()\n",
    "\n",
    "--- Stage 2: Parallel Canonical Code Assignment (CPU/GPU Hybrid) ---\n",
    "\n",
    "Canonical Huffman is used because tree traversal (non-canonical) is highly sequential.\n",
    "\n",
    "def canonical_code_assignment(freq_gpu):\n",
    "\"\"\"\n",
    "Hybrid stage: CPU calculates lengths (sequential), then GPU generates codes (parallel).\n",
    "\"\"\"\n",
    "freq_cpu = cp.asnumpy(freq_gpu)\n",
    "\n",
    "# 1. Sequential Code Length Determination (Huffman Algorithm)\n",
    "active_freq = {i: f for i, f in enumerate(freq_cpu) if f > 0}\n",
    "if not active_freq:\n",
    "    return cp.array([0]*MAX_SYMBOLS, dtype=cp.uint8), {}\n",
    "\n",
    "# Initial list of nodes: (frequency, symbol)\n",
    "heap = [[f, s] for s, f in active_freq.items()]\n",
    "heapq.heapify(heap)\n",
    "\n",
    "# Dictionary to store (symbol -> depth)\n",
    "depths = {s: 0 for s in active_freq.keys()}\n",
    "\n",
    "# Build the tree and record depths\n",
    "while len(heap) > 1:\n",
    "    lo_f, lo_s = heapq.heappop(heap)\n",
    "    hi_f, hi_s = heapq.heappop(heap)\n",
    "    \n",
    "    # Merge operation\n",
    "    new_s = (lo_s, hi_s) # Internal node represented by a tuple of children\n",
    "    new_f = lo_f + hi_f\n",
    "    heapq.heappush(heap, [new_f, new_s])\n",
    "    \n",
    "    # Increment depth of all children\n",
    "    for symbol_or_tuple in [lo_s, hi_s]:\n",
    "        if isinstance(symbol_or_tuple, int): # Leaf node (actual symbol)\n",
    "            depths[symbol_or_tuple] += 1\n",
    "        else: # Internal node (a tuple of children)\n",
    "            q = [symbol_or_tuple]\n",
    "            while q:\n",
    "                current = q.pop(0)\n",
    "                if isinstance(current, int):\n",
    "                    depths[current] += 1\n",
    "                else:\n",
    "                    q.extend(list(current))\n",
    "\n",
    "# 2. Parallel Canonical Code Generation (GPU)\n",
    "# The depth (length) is the only thing needed.\n",
    "code_lengths_cpu = np.zeros(MAX_SYMBOLS, dtype=np.uint8)\n",
    "for s, d in depths.items():\n",
    "    code_lengths_cpu[s] = d\n",
    "\n",
    "code_lengths_gpu = cp.asarray(code_lengths_cpu)\n",
    "\n",
    "# Sort symbols by length, then by symbol value (canonical order)\n",
    "# We will let the CPU handle the sorting of lengths and symbols, then generate codes on the GPU if complexity required it,\n",
    "# but since the generation itself is sequential once lengths are known, we'll keep the construction on the CPU for simplicity.\n",
    "\n",
    "# Final code generation (CPU-side for simplicity, but easily done on GPU)\n",
    "codes = {}\n",
    "\n",
    "# Get sorted symbols and lengths: [(length, symbol), ...]\n",
    "sorted_pairs = sorted([(code_lengths_cpu[i], i) for i in range(MAX_SYMBOLS) if code_lengths_cpu[i] > 0])\n",
    "\n",
    "if not sorted_pairs:\n",
    "    return code_lengths_gpu, {}\n",
    "\n",
    "current_code = 0\n",
    "current_length = 0\n",
    "\n",
    "for length, symbol in sorted_pairs:\n",
    "    if length > current_length:\n",
    "        if current_length > 0:\n",
    "            current_code = (current_code + 1) << (length - current_length)\n",
    "        current_length = length\n",
    "    \n",
    "    codes[symbol] = format(current_code, f'0{length}b')\n",
    "    current_code += 1\n",
    "\n",
    "return code_lengths_gpu, codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48140c45-1c0e-4636-89bd-490a9b875682",
   "metadata": {},
   "source": [
    "--- Stage 3: Parallel Bitstream Encoding (CUDA Kernel & CuPy Scan) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61394aca-905b-49b8-bbfd-c5cba6b82a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def parallel_write_bitstream(data_in, codes_gpu, code_lengths_gpu, start_bit_pos, output_bits, bitstream_size):\n",
    "\"\"\"\n",
    "Final CUDA kernel to write the packed bitstream.\n",
    "Each thread writes one full code at its pre-calculated start position.\n",
    "This kernel is complex due to bit-level memory addressing and potential cross-byte writes.\n",
    "\"\"\"\n",
    "idx = cuda.grid(1)\n",
    "\n",
    "if idx < data_in.size:\n",
    "    symbol = data_in[idx]\n",
    "    code_length = code_lengths_gpu[symbol]\n",
    "    \n",
    "    if code_length > 0:\n",
    "        # Code is stored as a 64-bit integer (max length 32 bits, but using 64 for safety)\n",
    "        # codes_gpu[symbol] contains the integer representation of the code\n",
    "        code_value = codes_gpu[symbol]\n",
    "        start_bit = start_bit_pos[idx]\n",
    "        \n",
    "        # Write the code bit by bit (simplified and safe approach)\n",
    "        for i in range(code_length):\n",
    "            # Calculate the position in the final bitstream array\n",
    "            current_bit_index = start_bit + i\n",
    "            byte_index = current_bit_index // 8\n",
    "            bit_offset = 7 - (current_bit_index % 8) # Write from MSB (bit 7) to LSB (bit 0)\n",
    "\n",
    "            # Check bounds safety (avoid writing beyond allocated output size)\n",
    "            if byte_index < bitstream_size:\n",
    "                # Extract the i-th bit from the code_value (reading from right to left, LSB to MSB)\n",
    "                bit_to_write = (code_value >> (code_length - 1 - i)) & 1\n",
    "                \n",
    "                # Atomic operation to safely set the bit in the output byte\n",
    "                if bit_to_write == 1:\n",
    "                    mask = 1 << bit_offset\n",
    "                    cuda.atomic.or_(output_bits, byte_index, mask)\n",
    "\n",
    "\n",
    "def gpu_encoding_stage(data_gpu, code_lengths_gpu, codes_dict):\n",
    "\"\"\"Wrapper for parallel encoding using CuPy for scan and Numba for write.\"\"\"\n",
    "N = data_gpu.size\n",
    "\n",
    "# Convert code dictionary to GPU arrays for lookup\n",
    "code_values_cpu = np.zeros(MAX_SYMBOLS, dtype=np.uint64)\n",
    "code_lengths_cpu = cp.asnumpy(code_lengths_gpu)\n",
    "\n",
    "for symbol, code_str in codes_dict.items():\n",
    "    # Store code value as an integer\n",
    "    code_values_cpu[symbol] = int(code_str, 2)\n",
    "\n",
    "codes_gpu = cp.asarray(code_values_cpu)\n",
    "\n",
    "# 1. Parallel Code Length Lookup\n",
    "# Get the length of each symbol's code for every symbol in the input data\n",
    "code_lengths_in_data = code_lengths_gpu[data_gpu]\n",
    "\n",
    "# 2. Parallel Prefix Sum (Scan)\n",
    "# Compute the starting bit position for each code using an exclusive scan\n",
    "# This is highly efficient in CuPy (and RAPIDS)\n",
    "start_bit_pos = cp.cuda.runtime.getDevice() # Ensure the scan runs on the selected device\n",
    "start_bit_pos = cp.cumsum(code_lengths_in_data, dtype=cp.uint64, axis=0)\n",
    "\n",
    "# Shift result for exclusive scan (starting index of the *current* code)\n",
    "start_bit_pos = cp.roll(start_bit_pos, 1)\n",
    "start_bit_pos[0] = 0\n",
    "\n",
    "total_bits = cp.sum(code_lengths_in_data).item()\n",
    "total_bytes = math.ceil(total_bits / 8)\n",
    "\n",
    "# 3. Parallel Bitstream Write\n",
    "output_bits = cp.zeros(total_bytes, dtype=cp.uint8)\n",
    "\n",
    "blocks_per_grid = (N + THREAD_PER_BLOCK - 1) // THREAD_PER_BLOCK\n",
    "\n",
    "# Launch the kernel to write the codes in parallel\n",
    "parallel_write_bitstream[blocks_per_grid, THREAD_PER_BLOCK](\n",
    "    data_gpu, codes_gpu, code_lengths_gpu, start_bit_pos, output_bits, total_bytes\n",
    ")\n",
    "cuda.synchronize()\n",
    "\n",
    "# Pad info for header\n",
    "padding_bits = (8 - (total_bits % 8)) % 8\n",
    "\n",
    "return output_bits, total_bits, padding_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a56e67-05b7-487c-bc10-34e855275f63",
   "metadata": {},
   "source": [
    "4. Multi-GPU / Multi-Core Parallelization Structure\n",
    "\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf72568-244e-4ec7-a2f8-30d1df2c983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelHuffman:\n",
    "def init(self, gpu_ids=[0]):\n",
    "self.gpu_ids = gpu_ids\n",
    "self.num_gpus = len(gpu_ids)\n",
    "self.devices = [cp.cuda.Device(i) for i in gpu_ids]\n",
    "\n",
    "def run_multi_gpu_encoding(self, data_cpu):\n",
    "    \"\"\"\n",
    "    Simulates Multi-GPU encoding by distributing the file chunks.\n",
    "    The overall parallelization is based on dividing the input data.\n",
    "    \"\"\"\n",
    "    N = data_cpu.size\n",
    "    chunk_size = math.ceil(N / self.num_gpus)\n",
    "    \n",
    "    chunk_data = []\n",
    "    for i in range(self.num_gpus):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, N)\n",
    "        chunk_data.append(data_cpu[start:end])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- Stage 1: Parallel Frequency Counting Across GPUs ---\n",
    "    total_freq_cpu = np.zeros(MAX_SYMBOLS, dtype=np.int32)\n",
    "    \n",
    "    for i, device in enumerate(self.devices):\n",
    "        with device:\n",
    "            # 1a. Copy chunk data to GPU i\n",
    "            data_gpu = cp.asarray(chunk_data[i])\n",
    "            freq_out_gpu = cp.zeros(MAX_SYMBOLS, dtype=cp.int32)\n",
    "            \n",
    "            # 1b. Run frequency counting kernel on GPU i\n",
    "            gpu_frequency_stage(data_gpu, freq_out_gpu)\n",
    "            \n",
    "            # 1c. Collect results (Atomic Merge in Global Memory is better, but here we merge on CPU/Host)\n",
    "            total_freq_cpu += cp.asnumpy(freq_out_gpu)\n",
    "    \n",
    "    # --- Stage 2: Code Assignment (Centralized on CPU) ---\n",
    "    # The tree construction (Code Assignment) is inherently centralized/sequential \n",
    "    # based on the total frequency map. It must be computed once on the host.\n",
    "    total_freq_gpu = cp.asarray(total_freq_cpu)\n",
    "    code_lengths_gpu_master, codes_dict_master = canonical_code_assignment(total_freq_gpu)\n",
    "    \n",
    "    # --- Stage 3: Parallel Bitstream Encoding Across GPUs ---\n",
    "    compressed_chunks = []\n",
    "    total_bits = 0\n",
    "    \n",
    "    for i, device in enumerate(self.devices):\n",
    "        with device:\n",
    "            data_gpu = cp.asarray(chunk_data[i])\n",
    "            \n",
    "            # Ensure the master codes are available on the device\n",
    "            code_lengths_gpu = code_lengths_gpu_master.copy()\n",
    "            \n",
    "            # Run encoding stage on GPU i\n",
    "            chunk_bits, chunk_total_bits, chunk_padding = gpu_encoding_stage(\n",
    "                data_gpu, code_lengths_gpu, codes_dict_master\n",
    "            )\n",
    "            \n",
    "            # Collect chunk results\n",
    "            compressed_chunks.append(cp.asnumpy(chunk_bits))\n",
    "            total_bits += chunk_total_bits\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Final Concatenation and Header\n",
    "    # The true challenge is the *bit-level* concatenation across chunks.\n",
    "    # Since we cannot perfectly bit-pack the last bits of chunk(i) with the first bits of chunk(i+1) in this simulation,\n",
    "    # we will use the most common parallel strategy: byte-aligning chunks (which adds minor overhead).\n",
    "    final_compressed_bytes = np.concatenate(compressed_chunks)\n",
    "    \n",
    "    # Re-calculate final padding based on total bits before padding was added.\n",
    "    final_padding = (8 - (total_bits % 8)) % 8\n",
    "    \n",
    "    header = str(final_padding) + '|' + str(codes_dict_master) + '|'\n",
    "    \n",
    "    # Assemble final result\n",
    "    result_bytes = header.encode('utf-8') + final_compressed_bytes.tobytes()\n",
    "    \n",
    "    return result_bytes, N, end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f330f11c-6119-4acf-ba10-bda48b3872dd",
   "metadata": {},
   "source": [
    "5. Evaluation Framework and Metrics\n",
    "\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafab68-0949-4ae4-a018-a1bb198f0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(name, runner_func, data, trials=3):\n",
    "\"\"\"Runs multiple trials and calculates average metrics.\"\"\"\n",
    "results = []\n",
    "\n",
    "for _ in range(trials):\n",
    "    # Time the operation\n",
    "    start_time = time.time()\n",
    "    compressed_data, original_size, elapsed_time = runner_func(data)\n",
    "    \n",
    "    # Metrics Calculation\n",
    "    compression_ratio = original_size / len(compressed_data)\n",
    "    throughput_mbps = (original_size / (1024 * 1024)) / elapsed_time\n",
    "    \n",
    "    results.append({\n",
    "        'throughput_mbps': throughput_mbps,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'elapsed_time': elapsed_time\n",
    "    })\n",
    "    \n",
    "avg_results = {\n",
    "    'name': name,\n",
    "    'avg_throughput': np.mean([r['throughput_mbps'] for r in results]),\n",
    "    'avg_ratio': np.mean([r['compression_ratio'] for r in results]),\n",
    "}\n",
    "return avg_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e8048-49ce-4170-ba22-ffd12c051ced",
   "metadata": {},
   "source": [
    "Wrapper function for sequential test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49bd53-e948-48d5-8d88-83d081af5f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_sequential_wrapper(data):\n",
    "compressed, original_size = cpu_huffman_encode(data)\n",
    "# The elapsed time is already included in the compressed data from cpu_huffman_encode\n",
    "# We will redefine the sequential wrapper for clean metrics collection.\n",
    "\n",
    "start = time.time()\n",
    "compressed, original_size = cpu_huffman_encode(data)\n",
    "end = time.time()\n",
    "\n",
    "# Return structure matching the run_multi_gpu_encoding output\n",
    "return compressed, original_size, end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a469e82-81fc-4ed1-aeb9-c29268f58677",
   "metadata": {},
   "source": [
    "Wrapper function for single-GPU test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8ea32-5daf-4e2f-94e6-6b8384cd41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_gpu_wrapper(data_cpu):\n",
    "# This uses the ParallelHuffman class initialized with one GPU (ID 0)\n",
    "huffman_runner = ParallelHuffman(gpu_ids=[0])\n",
    "return huffman_runner.run_multi_gpu_encoding(data_cpu)\n",
    "\n",
    "--- Main Benchmarking Function ---\n",
    "\n",
    "def benchmark_all_test_cases(file_size_mb=10, gpu_counts=[1, 2, 4]):\n",
    "\n",
    "print(f\"--- Running Benchmarks for File Size: {file_size_mb} MB ---\")\n",
    "data = generate_test_data(file_size_mb, redundancy_level='high')\n",
    "all_results = []\n",
    "\n",
    "# Test Case 1: Standard Sequential (One CPU)\n",
    "print(\"1. Testing Standard Sequential (CPU)...\")\n",
    "result_seq = run_trial(\"Sequential CPU\", run_sequential_wrapper, data)\n",
    "all_results.append(result_seq)\n",
    "print(f\"   -> Throughput: {result_seq['avg_throughput']:.2f} MB/s, Ratio: {result_seq['avg_ratio']:.3f}\")\n",
    "\n",
    "# Test Case 2: CPU Baseline Parallelization (Not explicitly coded, but the Numba/CuPy approach is faster than OpenMP/pthreads and serves as the primary acceleration target. We will focus on the GPU comparison.)\n",
    "# Note: For simplicity in this environment, we omit a separate OpenMP/pthreads implementation and focus on GPU vs. Sequential.\n",
    "\n",
    "# Test Case 3: Parallelized across GPU's (Multi-GPU, varying counts)\n",
    "for gpus in gpu_counts:\n",
    "    # Note: Set CUDA_VISIBLE_DEVICES environment variable before running this in a true terminal.\n",
    "    # In a Jupyter notebook, this will typically use the first N visible devices.\n",
    "    if gpus == 1:\n",
    "        name = \"Single GPU (ID 0)\"\n",
    "        print(f\"2. Testing {name}...\")\n",
    "        runner = run_single_gpu_wrapper\n",
    "    else:\n",
    "        name = f\"Multi-GPU ({gpus} GPUs)\"\n",
    "        print(f\"3. Testing {name}...\")\n",
    "        \n",
    "        # Ensure enough devices are visible/available on the server\n",
    "        try:\n",
    "            # Create the runner with the requested number of devices\n",
    "            runner = lambda d: ParallelHuffman(gpu_ids=list(range(gpus))).run_multi_gpu_encoding(d)\n",
    "        except cp.cuda.runtime.CudaAPIError:\n",
    "            print(f\"   -> WARNING: Could not initialize {gpus} GPUs. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    result_gpu = run_trial(name, runner, data)\n",
    "    all_results.append(result_gpu)\n",
    "    print(f\"   -> Throughput: {result_gpu['avg_throughput']:.2f} MB/s, Ratio: {result_gpu['avg_ratio']:.3f}\")\n",
    "    \n",
    "return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36730e-e339-428e-8445-cd43bbf9e2cd",
   "metadata": {},
   "source": [
    "6. Plotting Results\n",
    "\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3da9b-1f30-4c24-a9f4-4e85d555d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_list, filesize_mb):\n",
    "names = [r['name'] for r in results_list]\n",
    "throughputs = [r['avg_throughput'] for r in results_list]\n",
    "ratios = [r['avg_ratio'] for r in results_list]\n",
    "\n",
    "# Plot 1: Throughput (Scalability)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Calculate Speedup relative to the Sequential CPU baseline\n",
    "seq_throughput = throughputs[0]\n",
    "speedups = [t / seq_throughput for t in throughputs]\n",
    "\n",
    "plt.bar(names, throughputs, color=['gray'] + ['darkblue'] * (len(names)-1))\n",
    "plt.title(f'Throughput Comparison ({filesize_mb} MB File)', fontsize=14)\n",
    "plt.ylabel('Throughput (MB/s)')\n",
    "\n",
    "# Add Speedup text on top of the bars\n",
    "for i, speedup in enumerate(speedups):\n",
    "    if i == 0:\n",
    "        plt.text(i, throughputs[i] + 0.05 * max(throughputs), f'{1.00:.2f}x', ha='center', color='black')\n",
    "    else:\n",
    "        plt.text(i, throughputs[i] + 0.05 * max(throughputs), f'{speedup:.2f}x', ha='center', color='darkred', weight='bold')\n",
    "\n",
    "# Plot 2: Compression Ratio (Verification)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(names, ratios, color=['green'] * len(names))\n",
    "plt.title(f'Compression Ratio Verification ({filesize_mb} MB File)', fontsize=14)\n",
    "plt.ylabel('Compression Ratio (Original Size / Compressed Size)')\n",
    "plt.ylim(min(ratios) * 0.9, max(ratios) * 1.1)\n",
    "\n",
    "# Add ratio text\n",
    "for i, ratio in enumerate(ratios):\n",
    "    plt.text(i, ratios[i] + 0.005, f'{ratio:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7ca7d-7458-462c-9de1-a83351632e12",
   "metadata": {},
   "source": [
    "7. Execution Block\n",
    "\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc90f7-0f18-41c4-abe3-73393ddf57e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == 'main':\n",
    "# --- IMPORTANT ---\n",
    "# 1. Before running Multi-GPU tests, set the environment variable:\n",
    "#    E.g., In your Jupyter Terminal: export CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "# 2. You can vary these parameters for your trials:\n",
    "FILE_SIZE_TO_TEST = 50 # Start with 50MB, then increase to 500MB+ for better GPU utilization\n",
    "GPU_CONFIGS = [1, 2]  # Test 1 GPU and 2 GPUs (if available)\n",
    "\n",
    "# Run the full benchmark\n",
    "final_results = benchmark_all_test_cases(\n",
    "    file_size_mb=FILE_SIZE_TO_TEST,\n",
    "    gpu_counts=GPU_CONFIGS\n",
    ")\n",
    "\n",
    "# Generate the performance plots\n",
    "plot_results(final_results, FILE_SIZE_TO_TEST)\n",
    "\n",
    "print(\"\\n--- Project Conclusion Focus ---\")\n",
    "print(\"Your final report should analyze:\")\n",
    "print(\"1. **Throughput & Scalability:** How much speedup was achieved over the Sequential CPU baseline, and did performance scale linearly with GPU count?\")\n",
    "print(\"2. **Bottlenecks:** Which stage (Frequency Count, Code Assignment, or Bitstream Encoding/Decoding) was the slowest, and why (Hint: Code Assignment is inherently sequential; Bitstream Encoding is challenging due to complex bit-packing/unaligned memory access).\")\n",
    "print(\"3. **Compression Efficiency:** Verify that the Compression Ratio is consistent across all implementations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c176f1-4fe0-4eea-8b6e-2766cb046fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
